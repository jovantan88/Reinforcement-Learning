{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# Check if running in IPython for inline plots\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Device configuration: GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Named tuple for storing transitions\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Replay Memory for experience replay\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Q-Network\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_observations, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Initialise the MountainCar environment\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0     # Start with a higher epsilon\n",
    "EPS_END = 0.01      # Lower final epsilon\n",
    "EPS_DECAY = 500     # More gradual decay\n",
    "TAU = 0.01          # Soft update rate for target network\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 20000\n",
    "NUM_EPISODES = 800  # Increased episodes for better exploration\n",
    "\n",
    "# Retrieve number of actions and state dimension\n",
    "n_actions = env.action_space.n\n",
    "state, info = env.reset()\n",
    "n_observations = len(state)\n",
    "\n",
    "# Initialise Policy Network and Target Network (for stability)\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.95)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"Epsilon-greedy policy\"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    # Compute decayed epsilon\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Non-final mask and state batches\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(s is not None for s in batch.next_state),\n",
    "        device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Current Q values from policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # SINGLE DQN: Use target_net to evaluate next state values, directly applying max\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        q_values_next = target_net(non_final_next_states)\n",
    "        next_state_values[non_final_mask] = q_values_next.max(1)[0]  # Single DQN approach\n",
    "\n",
    "    # Expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber Loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 10)\n",
    "    optimizer.step()\n",
    "\n",
    "# Tracking rewards\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Training Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f'Training... Episode {len(episode_rewards)}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Reward per Episode')\n",
    "    # 100-episode average\n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100-Episode Average')\n",
    "        plt.legend()\n",
    "    plt.pause(0.001)\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "# Main training loop\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        action = select_action(state)\n",
    "        next_state_raw, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        # Reward shaping\n",
    "        position, velocity = next_state_raw\n",
    "        shaped_reward = position + 0.5  # Encourages moving right\n",
    "        if position >= env.goal_position:  \n",
    "            shaped_reward += 10.0        # Extra reward for achieving the goal\n",
    "\n",
    "        total_reward += shaped_reward\n",
    "\n",
    "        done = terminated or truncated\n",
    "        next_state = None if done else torch.tensor([next_state_raw], device=device, dtype=torch.float32)\n",
    "\n",
    "        # Store transition in replay memory\n",
    "        memory.push(state, action, next_state, torch.tensor([shaped_reward], device=device))\n",
    "\n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "        # Optimise the model\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target_net\n",
    "        for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(total_reward)\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "    # Update learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training Complete\")\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# Optionally save the best model\n",
    "if episode_rewards and max(episode_rewards) == episode_rewards[-1]:\n",
    "    torch.save(policy_net.state_dict(), 'best_model_mountain_car.pth')\n",
    "\n",
    "# Demonstration/Recording\n",
    "from gym.wrappers import RecordVideo\n",
    "simulation_env = gym.make('MountainCar-v0', render_mode=\"rgb_array\")\n",
    "simulation_env = RecordVideo(simulation_env, './video_mountain_car', episode_trigger=lambda e_idx: True)\n",
    "\n",
    "model = DQN(n_observations, n_actions).to(device)\n",
    "model.load_state_dict(torch.load('best_model_mountain_car.pth'))\n",
    "model.eval()\n",
    "\n",
    "state, _ = simulation_env.reset()\n",
    "state = torch.tensor([state], device=device, dtype=torch.float32)\n",
    "done = False\n",
    "while not done:\n",
    "    with torch.no_grad():\n",
    "        action = model(state).argmax(dim=1).view(1, 1)\n",
    "    next_state_raw, _, terminated, truncated, _ = simulation_env.step(action.item())\n",
    "    next_state = torch.tensor([next_state_raw], device=device, dtype=torch.float32)\n",
    "    state = next_state\n",
    "    done = terminated or truncated\n",
    "\n",
    "simulation_env.close()\n",
    "print(\"Video recorded. Check the 'video_mountain_car' folder for the output.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# For inline plotting (Jupyter notebooks)\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# Device: use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Named tuple to store transitions for replay\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Replay Memory for experience replay\n",
    "class ReplayMemory:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Store a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "# Single DQN architecture\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_observations, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Create the environment in \"rgb_array\" mode for video recording\n",
    "# Wrap it with RecordVideo, triggering recording every 50 episodes\n",
    "from gym.wrappers import RecordVideo\n",
    "env = gym.make(\"MountainCar-v0\", render_mode=\"rgb_array\")\n",
    "env = RecordVideo(\n",
    "    env, \n",
    "    video_folder='./video_mountain_car', \n",
    "    episode_trigger=lambda episode_id: (episode_id + 1) % 50 == 0\n",
    ")\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0     # Start with a high epsilon for exploration\n",
    "EPS_END = 0.01      # Final epsilon after decay\n",
    "EPS_DECAY = 500     # Decay rate\n",
    "TAU = 0.01          # Soft update rate for the target network\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 20000\n",
    "NUM_EPISODES = 800  # Enough episodes to see improvements\n",
    "\n",
    "# Determine action & state dimensions\n",
    "initial_state, info = env.reset()\n",
    "n_actions = env.action_space.n\n",
    "n_observations = len(initial_state)\n",
    "\n",
    "# Create the policy and target networks\n",
    "policy_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net = DQN(n_observations, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayMemory(MEMORY_SIZE)\n",
    "scheduler = StepLR(optimizer, step_size=50, gamma=0.95)\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    \"\"\"Epsilon-greedy strategy.\"\"\"\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1.0 * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax(dim=1).view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "def optimize_model():\n",
    "    \"\"\"Perform one step of optimisation.\"\"\"\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Construct batch tensors\n",
    "    non_final_mask = torch.tensor(\n",
    "        tuple(s is not None for s in batch.next_state),\n",
    "        device=device, dtype=torch.bool\n",
    "    )\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Compute Q(s, a) using policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # SINGLE DQN: directly use target_net for next state\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        q_values_next = target_net(non_final_next_states)\n",
    "        next_state_values[non_final_mask] = q_values_next.max(1)[0]\n",
    "\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # Optional gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 10)\n",
    "    optimizer.step()\n",
    "\n",
    "# Track the rewards for plotting\n",
    "episode_rewards = []\n",
    "\n",
    "def plot_rewards(show_result=False):\n",
    "    \"\"\"Plot episode rewards and 100-episode average.\"\"\"\n",
    "    plt.figure(1)\n",
    "    rewards_t = torch.tensor(episode_rewards, dtype=torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Training Results')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title(f'Training... Episode {len(episode_rewards)}')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(rewards_t.numpy(), label='Reward per Episode')\n",
    "    \n",
    "    if len(rewards_t) >= 100:\n",
    "        means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy(), label='100-Episode Average')\n",
    "        plt.legend()\n",
    "    plt.pause(0.001)\n",
    "\n",
    "    if is_ipython:\n",
    "        if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            display.display(plt.gcf())\n",
    "\n",
    "# Main training loop\n",
    "for i_episode in range(NUM_EPISODES):\n",
    "    # Reset environment and state\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.tensor([obs], device=device, dtype=torch.float32)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        # Select action\n",
    "        action = select_action(obs)\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
    "\n",
    "        # Reward shaping: nudge to move right & big bonus on goal\n",
    "        position, velocity = next_obs\n",
    "        shaped_reward = position + 0.5\n",
    "        if position >= env.goal_position:\n",
    "            shaped_reward += 10.0\n",
    "\n",
    "        total_reward += shaped_reward\n",
    "\n",
    "        done = terminated or truncated\n",
    "        next_obs = None if done else torch.tensor([next_obs], device=device, dtype=torch.float32)\n",
    "\n",
    "        # Store in replay memory\n",
    "        memory.push(obs, action, next_obs, torch.tensor([shaped_reward], device=device))\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "        # Optimize\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of target_net\n",
    "        for target_param, policy_param in zip(target_net.parameters(), policy_net.parameters()):\n",
    "            target_param.data.copy_(TAU * policy_param.data + (1.0 - TAU) * target_param.data)\n",
    "\n",
    "        if done:\n",
    "            episode_rewards.append(total_reward)\n",
    "            plot_rewards()\n",
    "            break\n",
    "\n",
    "    # Step the LR scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Training Complete\")\n",
    "plot_rewards(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "\n",
    "# Optionally, save the best-performing policy\n",
    "if episode_rewards and max(episode_rewards) == episode_rewards[-1]:\n",
    "    torch.save(policy_net.state_dict(), 'best_model_mountain_car.pth')\n",
    "\n",
    "# Close the environment gracefully\n",
    "env.close()\n",
    "print(\"Videos saved every 50 episodes in the 'video_mountain_car' folder.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
